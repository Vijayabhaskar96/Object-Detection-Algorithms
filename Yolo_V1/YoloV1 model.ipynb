{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensor\n",
    "from dataset import YoloV1DataModule\n",
    "from utils import get_bboxesmine, intersection_over_union, mAP\n",
    "import configs\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_config = namedtuple(\"ConvConfig\",[\"kernel_size\",\"filters\",\"stride\",\"pad\"])\n",
    "maxpool_config = namedtuple(\"MaxPoolConfig\",[\"kernel_size\",\"stride\"])\n",
    "repeat_block = namedtuple(\"Repeat\",[\"blocks\",\"n\"])\n",
    "architechture_config = [conv_config(7, 64, 2, 3),\n",
    "                        maxpool_config(2, 2),\n",
    "                        conv_config(3, 192, 1, 1),\n",
    "                        maxpool_config(2, 2),\n",
    "                        conv_config(1, 128, 1, 0),\n",
    "                        conv_config(3, 256, 1, 1),\n",
    "                        conv_config(1, 256, 1, 0),\n",
    "                        conv_config(3, 512, 1, 1),\n",
    "                        maxpool_config(2, 2),\n",
    "                        repeat_block([conv_config(1, 256, 1, 0), conv_config(3, 512, 1, 1)], 4),\n",
    "                        conv_config(1, 512, 1, 0),\n",
    "                        conv_config(3, 1024, 1, 1),\n",
    "                        maxpool_config(2, 2),\n",
    "                        repeat_block([conv_config(1, 512, 1, 0), conv_config(3, 1024, 1, 1)], 2),\n",
    "                        conv_config(3, 1024, 1, 1),\n",
    "                        conv_config(3, 1024, 2, 1),\n",
    "                        conv_config(3, 1024, 1, 1),\n",
    "                        conv_config(3, 1024, 1, 1)\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper and VOC dataset is 20),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 20].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (bestbox * predictions[..., 26:30] + (1 - bestbox) * predictions[..., 21:25])\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        # Take sqrt of width, height of boxes to ensure that\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(torch.abs(box_predictions[..., 2:4] + 1e-6))\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(torch.flatten(box_predictions, end_dim=-2),\n",
    "                            torch.flatten(box_targets, end_dim=-2))\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21])\n",
    "        pred_box = iou_maxes * pred_box\n",
    "        object_loss = self.mse(torch.flatten(exists_box * pred_box),\n",
    "                               torch.flatten(exists_box * target[..., 20:21]))\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1))\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (self.lambda_coord * box_loss  # first two rows in paper\n",
    "                + object_loss  # third row in paper\n",
    "                + self.lambda_noobj * no_object_loss  # forth row\n",
    "                + class_loss  # fifth row\n",
    "               )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters, kernel_size, stride, pad):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels = in_channels,\n",
    "                              out_channels = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              stride = stride,\n",
    "                              padding = pad,\n",
    "                              bias = False)\n",
    "        self.batchnorm = nn.BatchNorm2d(filters)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloV1Model(pl.LightningModule):\n",
    "    def __init__(self, in_channels=3, architechture=None, split_size=7, num_boxes=2, num_classes=20):\n",
    "        super(YoloV1Model, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv(architechture)\n",
    "        self.fcs = self._create_fcs(split_size, num_boxes, num_classes)\n",
    "        self.loss = YoloLoss()\n",
    "        self.train_transform = A.Compose([A.Resize(width=448, height=448),\n",
    "                                          A.ShiftScaleRotate(shift_limit=0.2,scale_limit=0.2,rotate_limit=0),ToTensor()],\n",
    "                                    bbox_params=A.BboxParams(format='yolo',label_fields=['class_labels']))\n",
    "        self.test_transform = A.Compose([A.Resize(width=448, height=448),ToTensor()],\n",
    "                                    bbox_params=A.BboxParams(format='yolo',label_fields=['class_labels']))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "        \n",
    "    def _create_conv(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if \"Conv\" in str(type(x)):\n",
    "                layers += [CNNBlock(in_channels = in_channels,\n",
    "                                    filters = x.filters,\n",
    "                                    kernel_size = x.kernel_size,\n",
    "                                    stride = x.stride,\n",
    "                                    pad = x.pad)\n",
    "                          ]\n",
    "                in_channels = x.filters\n",
    "\n",
    "            elif \"MaxPool\" in str(type(x)):\n",
    "                layers += [nn.MaxPool2d(kernel_size=(x.kernel_size, x.kernel_size), stride=(x.stride, x.stride))]\n",
    "\n",
    "            elif \"Repeat\" in str(type(x)):\n",
    "                convs = x.blocks\n",
    "                num_repeats = x.n\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    for conv in convs:\n",
    "                        layers += [CNNBlock(in_channels,\n",
    "                                            conv.filters,\n",
    "                                            kernel_size=conv.kernel_size,\n",
    "                                            stride=conv.stride,\n",
    "                                            pad=conv.pad)\n",
    "                                  ]\n",
    "                        in_channels = conv.filters\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        self.S, self.B, self.C = split_size, num_boxes, num_classes\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * self.S * self.S, 4096),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, self.S * self.S * (self.C + self.B * 5)))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=configs.LEARNING_RATE, weight_decay=configs.WEIGHT_DECAY)\n",
    "        return {'optimizer': optimizer}\n",
    "\n",
    "    def _calc_map(self, x, y, pred):\n",
    "        pred_boxes, target_boxes = get_bboxesmine(x=x,y=y,predictions=pred,iou_threshold=0.5, threshold=0.4, S=self.S, device=self.device)\n",
    "        mean_avg_prec = mAP(pred_boxes,target_boxes,iou_threshold=0.5)\n",
    "        return mean_avg_prec\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred_y = self(x)\n",
    "        loss = self.loss(pred_y, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        mAP = self._calc_map(x.detach(), y.detach(), pred_y.detach())\n",
    "        self.log('train_mAP', mAP, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred_y = self(x)\n",
    "        loss = self.loss(pred_y, y)\n",
    "        self.log('valid_loss', loss, prog_bar=True)\n",
    "        mAP = self._calc_map(x.detach(), y.detach(), pred_y.detach())\n",
    "        self.log('valid_mAP', mAP,prog_bar=True)\n",
    "        return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | darknet | Sequential | 60 M  \n",
      "1 | fcs     | Sequential | 25 M  \n",
      "2 | loss    | YoloLoss   | 0     \n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I1107 23:59:43.537468  3960 lightning.py:1295] \n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | darknet | Sequential | 60 M  \n",
      "1 | fcs     | Sequential | 25 M  \n",
      "2 | loss    | YoloLoss   | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1606c6c526b4e79aa438c03117ffdcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\winpy3710\\python-3.7.1.amd64\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YoloV1Model(architechture = architechture_config, split_size=7, num_boxes=2, num_classes=20)\n",
    "data = YoloV1DataModule()\n",
    "trainer = pl.Trainer(gpus=1,max_epochs=20)\n",
    "trainer.fit(model,datamodule=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
