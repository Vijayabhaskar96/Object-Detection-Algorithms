{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo V1 implementation in Pytorch\n",
    "\n",
    "\n",
    "References:\n",
    "* Redmon J, Divvala S, Girshick R, Farhadi A (2016) You only look once: Unified, real-time object detection. Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit 2016-Decem:779–788 . https://doi.org/10.1109/CVPR.2016.91\n",
    "* Aladdin Persson's Playlist https://www.youtube.com/playlist?list=PLhhyoLH6Ijfw0TpCTVTNk42NN08H6UvNq\n",
    "* R. Padilla, S. L. Netto and E. A. B. da Silva, “A Survey on Performance Metrics for Object-Detection Algorithms,” 2020 International Conference on Systems, Signals and Image Processing (IWSSIP), Niterói, Brazil, 2020, pp. 237–242, doi: 10.1109/IWSSIP48289.2020.9145130.\n",
    "PDF available in the next reference repo.\n",
    "* https://github.com/rafaelpadilla/Object-Detection-Metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# kaggle_data = {\n",
    "#     \"username\": \"ENTER_YOUR_KAGGLE_USERNAME_HERE\",\n",
    "#     \"key\": \"ENTER_YOUR_KAGGLE_KEY_HERE\",\n",
    "# }\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = kaggle_data[\"username\"]\n",
    "# os.environ[\"KAGGLE_KEY\"] = kaggle_data[\"key\"]\n",
    "# !pip install pytorch-lightning\n",
    "# !pip install kaggle\n",
    "# !pip install --upgrade albumentations\n",
    "# !wget https://raw.githubusercontent.com/pjreddie/darknet/master/scripts/voc_label.py\n",
    "# !kaggle datasets download -d vijayabhaskar96/pascal-voc-2007-and-2012\n",
    "# !unzip pascal-voc-2007-and-2012.zip\n",
    "# %run voc_label.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensor\n",
    "from dataset import YoloV1DataModule\n",
    "from utils import get_bboxes, intersection_over_union, mAP\n",
    "import configs\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_config = namedtuple(\"ConvConfig\", [\"kernel_size\", \"filters\", \"stride\", \"pad\"])\n",
    "maxpool_config = namedtuple(\"MaxPoolConfig\", [\"kernel_size\", \"stride\"])\n",
    "repeat_block = namedtuple(\"Repeat\", [\"blocks\", \"n\"])\n",
    "architechture_config = [\n",
    "    conv_config(7, 64, 2, 3),\n",
    "    maxpool_config(2, 2),\n",
    "    conv_config(3, 192, 1, 1),\n",
    "    maxpool_config(2, 2),\n",
    "    conv_config(1, 128, 1, 0),\n",
    "    conv_config(3, 256, 1, 1),\n",
    "    conv_config(1, 256, 1, 0),\n",
    "    conv_config(3, 512, 1, 1),\n",
    "    maxpool_config(2, 2),\n",
    "    repeat_block([conv_config(1, 256, 1, 0), conv_config(3, 512, 1, 1)], 4),\n",
    "    conv_config(1, 512, 1, 0),\n",
    "    conv_config(3, 1024, 1, 1),\n",
    "    maxpool_config(2, 2),\n",
    "    repeat_block([conv_config(1, 512, 1, 0), conv_config(3, 1024, 1, 1)], 2),\n",
    "    conv_config(3, 1024, 1, 1),\n",
    "    conv_config(3, 1024, 2, 1),\n",
    "    conv_config(3, 1024, 1, 1),\n",
    "    conv_config(3, 1024, 1, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper and VOC dataset is 20),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 20].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two\n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            bestbox * predictions[..., 26:30] + (1 - bestbox) * predictions[..., 21:25]\n",
    "        )\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        # Take sqrt of width, height of boxes to ensure that\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
    "        )\n",
    "        pred_box = iou_maxes * pred_box\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 20:21]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(\n",
    "                exists_box * predictions[..., :20],\n",
    "                end_dim=-2,\n",
    "            ),\n",
    "            torch.flatten(\n",
    "                exists_box * target[..., :20],\n",
    "                end_dim=-2,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters, kernel_size, stride, pad):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=pad,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.batchnorm = nn.BatchNorm2d(filters)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloV1Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        architechture=None,\n",
    "        split_size=7,\n",
    "        num_boxes=2,\n",
    "        num_classes=20,\n",
    "    ):\n",
    "        super(YoloV1Model, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv(architechture)\n",
    "        self.fcs = self._create_fcs(split_size, num_boxes, num_classes)\n",
    "        self.train_transform = A.Compose(\n",
    "            [\n",
    "                A.Resize(width=448, height=448),\n",
    "                A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=0),\n",
    "                ToTensor(),\n",
    "            ],\n",
    "            bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"]),\n",
    "        )\n",
    "        self.test_transform = A.Compose(\n",
    "            [A.Resize(width=448, height=448), ToTensor()],\n",
    "            bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "\n",
    "    def _create_conv(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if \"Conv\" in str(type(x)):\n",
    "                layers += [\n",
    "                    CNNBlock(\n",
    "                        in_channels=in_channels,\n",
    "                        filters=x.filters,\n",
    "                        kernel_size=x.kernel_size,\n",
    "                        stride=x.stride,\n",
    "                        pad=x.pad,\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x.filters\n",
    "\n",
    "            elif \"MaxPool\" in str(type(x)):\n",
    "                layers += [\n",
    "                    nn.MaxPool2d(\n",
    "                        kernel_size=(x.kernel_size, x.kernel_size),\n",
    "                        stride=(x.stride, x.stride),\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            elif \"Repeat\" in str(type(x)):\n",
    "                convs = x.blocks\n",
    "                num_repeats = x.n\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    for conv in convs:\n",
    "                        layers += [\n",
    "                            CNNBlock(\n",
    "                                in_channels,\n",
    "                                conv.filters,\n",
    "                                kernel_size=conv.kernel_size,\n",
    "                                stride=conv.stride,\n",
    "                                pad=conv.pad,\n",
    "                            )\n",
    "                        ]\n",
    "                        in_channels = conv.filters\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        self.S, self.B, self.C = split_size, num_boxes, num_classes\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * self.S * self.S, 496),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(496, self.S * self.S * (self.C + self.B * 5)),\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=configs.LEARNING_RATE,\n",
    "            weight_decay=configs.WEIGHT_DECAY,\n",
    "        )\n",
    "        return {\"optimizer\": optimizer}\n",
    "\n",
    "    def _calc_map(self, x, y, pred):\n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            predictions=pred,\n",
    "            iou_threshold=0.5,\n",
    "            threshold=0.4,\n",
    "            S=self.S,\n",
    "            device=self.device,\n",
    "        )\n",
    "        mean_avg_prec = mAP(pred_boxes, target_boxes, iou_threshold=0.5)\n",
    "        return mean_avg_prec\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred_y = self(x)\n",
    "        loss = YoloLoss()(pred_y, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        with torch.no_grad():\n",
    "            mAP = self._calc_map(x, y, pred_y)\n",
    "            self.log(\"train_mAP\", mAP, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred_y = self(x)\n",
    "        loss = YoloLoss()(pred_y, y)\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        mAP = self._calc_map(x, y, pred_y)\n",
    "        self.log(\"valid_mAP\", mAP, prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = YoloV1Model(\n",
    "        architechture=architechture_config, split_size=7, num_boxes=2, num_classes=20\n",
    "    )\n",
    "    data = YoloV1DataModule()\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=1000)\n",
    "    trainer.fit(model, datamodule=data)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
