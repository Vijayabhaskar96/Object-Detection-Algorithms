{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo V1 implementation in Pytorch\n",
    "\n",
    "\n",
    "References:\n",
    "* Redmon J, Divvala S, Girshick R, Farhadi A (2016) You only look once: Unified, real-time object detection. Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit 2016-Decem:779–788 . https://doi.org/10.1109/CVPR.2016.91\n",
    "* Aladdin Persson's Playlist https://www.youtube.com/playlist?list=PLhhyoLH6Ijfw0TpCTVTNk42NN08H6UvNq\n",
    "* R. Padilla, S. L. Netto and E. A. B. da Silva, “A Survey on Performance Metrics for Object-Detection Algorithms,” 2020 International Conference on Systems, Signals and Image Processing (IWSSIP), Niterói, Brazil, 2020, pp. 237–242, doi: 10.1109/IWSSIP48289.2020.9145130.\n",
    "PDF available in the next reference repo.\n",
    "* https://github.com/rafaelpadilla/Object-Detection-Metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "kaggle_data={\"username\":\"ENTER_YOUR_KAGGLE_USERNAME_HERE\",\"key\":\"ENTER_YOUR_KAGGLE_KEY_HERE\"}\n",
    "os.environ['KAGGLE_USERNAME']=kaggle_data[\"username\"]\n",
    "os.environ['KAGGLE_KEY']=kaggle_data[\"key\"]\n",
    "!pip install pytorch-lightning\n",
    "!pip install kaggle\n",
    "!pip install --upgrade albumentations\n",
    "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/scripts/voc_label.py\n",
    "!kaggle datasets download -d vijayabhaskar96/pascal-voc-2007-and-2012\n",
    "!unzip pascal-voc-2007-and-2012.zip\n",
    "%run voc_label.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensor\n",
    "from dataset import YoloV1DataModule\n",
    "from utils import get_bboxesmine, intersection_over_union, mAP\n",
    "import configs\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_config = namedtuple(\"ConvConfig\",[\"kernel_size\",\"filters\",\"stride\",\"pad\"])\n",
    "maxpool_config = namedtuple(\"MaxPoolConfig\",[\"kernel_size\",\"stride\"])\n",
    "repeat_block = namedtuple(\"Repeat\",[\"blocks\",\"n\"])\n",
    "architechture_config = [conv_config(7, 64, 2, 3),\n",
    "                        maxpool_config(2, 2),\n",
    "                        conv_config(3, 192, 1, 1),\n",
    "                        maxpool_config(2, 2),\n",
    "                        conv_config(1, 128, 1, 0),\n",
    "                        conv_config(3, 256, 1, 1),\n",
    "                        conv_config(1, 256, 1, 0),\n",
    "                        conv_config(3, 512, 1, 1),\n",
    "                        maxpool_config(2, 2),\n",
    "                        repeat_block([conv_config(1, 256, 1, 0), conv_config(3, 512, 1, 1)], 4),\n",
    "                        conv_config(1, 512, 1, 0),\n",
    "                        conv_config(3, 1024, 1, 1),\n",
    "                        maxpool_config(2, 2),\n",
    "                        repeat_block([conv_config(1, 512, 1, 0), conv_config(3, 1024, 1, 1)], 2),\n",
    "                        conv_config(3, 1024, 1, 1),\n",
    "                        conv_config(3, 1024, 2, 1),\n",
    "                        conv_config(3, 1024, 1, 1),\n",
    "                        conv_config(3, 1024, 1, 1)\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper and VOC dataset is 20),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 20].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (bestbox * predictions[..., 26:30] + (1 - bestbox) * predictions[..., 21:25])\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        # Take sqrt of width, height of boxes to ensure that\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(torch.abs(box_predictions[..., 2:4] + 1e-6))\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(torch.flatten(box_predictions, end_dim=-2),\n",
    "                            torch.flatten(box_targets, end_dim=-2))\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21])\n",
    "        pred_box = iou_maxes * pred_box\n",
    "        object_loss = self.mse(torch.flatten(exists_box * pred_box),\n",
    "                               torch.flatten(exists_box * target[..., 20:21]))\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1))\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (self.lambda_coord * box_loss  # first two rows in paper\n",
    "                + object_loss  # third row in paper\n",
    "                + self.lambda_noobj * no_object_loss  # forth row\n",
    "                + class_loss  # fifth row\n",
    "               )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters, kernel_size, stride, pad):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels = in_channels,\n",
    "                              out_channels = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              stride = stride,\n",
    "                              padding = pad,\n",
    "                              bias = False)\n",
    "        self.batchnorm = nn.BatchNorm2d(filters)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloV1Model(pl.LightningModule):\n",
    "    def __init__(self, in_channels=3, architechture=None, split_size=7, num_boxes=2, num_classes=20):\n",
    "        super(YoloV1Model, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv(architechture)\n",
    "        self.fcs = self._create_fcs(split_size, num_boxes, num_classes)\n",
    "        self.train_transform = A.Compose([A.Resize(width=448, height=448),\n",
    "                                          A.ShiftScaleRotate(shift_limit=0.2,scale_limit=0.2,rotate_limit=0),ToTensor()],\n",
    "                                    bbox_params=A.BboxParams(format='yolo',label_fields=['class_labels']))\n",
    "        self.test_transform = A.Compose([A.Resize(width=448, height=448),ToTensor()],\n",
    "                                    bbox_params=A.BboxParams(format='yolo',label_fields=['class_labels']))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "        \n",
    "    def _create_conv(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if \"Conv\" in str(type(x)):\n",
    "                layers += [CNNBlock(in_channels = in_channels,\n",
    "                                    filters = x.filters,\n",
    "                                    kernel_size = x.kernel_size,\n",
    "                                    stride = x.stride,\n",
    "                                    pad = x.pad)\n",
    "                          ]\n",
    "                in_channels = x.filters\n",
    "\n",
    "            elif \"MaxPool\" in str(type(x)):\n",
    "                layers += [nn.MaxPool2d(kernel_size=(x.kernel_size, x.kernel_size), stride=(x.stride, x.stride))]\n",
    "\n",
    "            elif \"Repeat\" in str(type(x)):\n",
    "                convs = x.blocks\n",
    "                num_repeats = x.n\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    for conv in convs:\n",
    "                        layers += [CNNBlock(in_channels,\n",
    "                                            conv.filters,\n",
    "                                            kernel_size=conv.kernel_size,\n",
    "                                            stride=conv.stride,\n",
    "                                            pad=conv.pad)\n",
    "                                  ]\n",
    "                        in_channels = conv.filters\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        self.S, self.B, self.C = split_size, num_boxes, num_classes\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * self.S * self.S, 4096),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, self.S * self.S * (self.C + self.B * 5)))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=configs.LEARNING_RATE, weight_decay=configs.WEIGHT_DECAY)\n",
    "        return {'optimizer': optimizer}\n",
    "\n",
    "    def _calc_map(self, x, y, pred):\n",
    "        pred_boxes, target_boxes = get_bboxesmine(x=x,y=y,predictions=pred,iou_threshold=0.5, threshold=0.4, S=self.S, device=self.device)\n",
    "        mean_avg_prec = mAP(pred_boxes,target_boxes,iou_threshold=0.5)\n",
    "        return mean_avg_prec\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred_y = self(x)\n",
    "        loss = YoloLoss()(pred_y, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        with torch.no_grad():\n",
    "            mAP = self._calc_map(x, y, pred_y)\n",
    "            self.log('train_mAP', mAP, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred_y = self(x)\n",
    "        loss = YoloLoss()(pred_y, y)\n",
    "        self.log('valid_loss', loss, prog_bar=True)\n",
    "        mAP = self._calc_map(x, y, pred_y)\n",
    "        self.log('valid_mAP', mAP,prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YoloV1Model(architechture = architechture_config, split_size=7, num_boxes=2, num_classes=20)\n",
    "data = YoloV1DataModule()\n",
    "trainer = pl.Trainer(gpus=1,overfit_batches=1,checkpoint_callback=False,max_epochs=1000)\n",
    "trainer.fit(model,datamodule=data)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
