{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo V2 implementation in Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# kaggle_data={\"username\":\"ENTER_YOUR_KAGGLE_USERNAME_HERE\",\"key\":\"ENTER_YOUR_KAGGLE_KEY_HERE\"}\n",
    "# os.environ['KAGGLE_USERNAME']=kaggle_data[\"username\"]\n",
    "# os.environ['KAGGLE_KEY']=kaggle_data[\"key\"]\n",
    "# !pip install pytorch-lightning\n",
    "# !pip install kaggle\n",
    "# !pip install --upgrade albumentations\n",
    "# !wget https://raw.githubusercontent.com/pjreddie/darknet/master/scripts/voc_label.py\n",
    "# !kaggle datasets download -d vijayabhaskar96/pascal-voc-2007-and-2012\n",
    "# !unzip pascal-voc-2007-and-2012.zip\n",
    "# %run voc_label.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from dataset import YoloV2DataModule\n",
    "from utils import get_bboxesmine, intersection_over_union, mAP\n",
    "import configs\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "%matplotlib inline\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_config = namedtuple(\"ConvConfig\",[\"kernel_size\",\"filters\",\"stride\",\"pad\"])\n",
    "maxpool_config = namedtuple(\"MaxPoolConfig\",[\"kernel_size\",\"stride\"])\n",
    "architechture_config1 = [conv_config(3, 32, 1, 1),\n",
    "                        maxpool_config(2, 2),\n",
    "                        conv_config(3, 64, 1, 1),\n",
    "                        maxpool_config(2, 2),\n",
    "                        conv_config(3, 128, 1, 1),\n",
    "                        conv_config(1, 64, 1, 0),\n",
    "                        conv_config(3, 128, 1, 1),\n",
    "                        maxpool_config(2, 2),\n",
    "                        conv_config(3, 256, 1, 1),\n",
    "                        conv_config(1, 128, 1, 0),\n",
    "                        conv_config(3, 256, 1, 1),\n",
    "                        maxpool_config(2, 2),\n",
    "                        conv_config(3, 512, 1, 1),\n",
    "                        conv_config(1, 256, 1, 0),\n",
    "                        conv_config(3, 512, 1, 1),\n",
    "                        conv_config(1, 256, 1, 0),\n",
    "                        conv_config(3, 512, 1, 1),\n",
    "                        ]\n",
    "architechture_config2 = [maxpool_config(2, 2),\n",
    "                        conv_config(3, 1024, 1, 1),\n",
    "                        conv_config(1, 512, 1, 0),\n",
    "                        conv_config(3, 1024, 1, 1),\n",
    "                        conv_config(1, 512, 1, 0),\n",
    "                        conv_config(3, 1024, 1, 1),\n",
    "                        conv_config(3, 1024, 1, 1),\n",
    "                        conv_config(3, 1024, 1, 1)\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloV2Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v2) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=13, B=5, C=20):\n",
    "        super(YoloV2Loss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "        self.anchor_boxes = torch.tensor([[0,0,1.3221, 1.73145],[0,0,3.19275, 4.00944],[0,0,5.05587, 8.09892],[0,0,9.47112, 4.84053],[0,0,11.2364, 10.0071]])\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper and VOC dataset is 20),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "\n",
    "    def forward(self, predictions, target, device,epoch=0):\n",
    "        self.anchor_boxes = self.anchor_boxes.to(device)\n",
    "        exist_mask = target[...,4:5]\n",
    "        existing_boxes = exist_mask * predictions\n",
    "        cell_idx = torch.arange(13,device=device)\n",
    "        bx = exist_mask*torch.sigmoid(predictions[...,0:1]) + exist_mask*cell_idx.view([1,1,-1,1,1])\n",
    "        by = exist_mask*torch.sigmoid(predictions[...,1:2]) + exist_mask*cell_idx.view([1,-1,1,1,1])\n",
    "        bw = exist_mask*self.anchor_boxes[:,2].view([1,1,1,-1,1]) * exist_mask*torch.exp(predictions[...,2:3])\n",
    "        bh = exist_mask*self.anchor_boxes[:,3].view([1,1,1,-1,1]) * exist_mask*torch.exp(predictions[...,3:4])\n",
    "\n",
    "        ious = intersection_over_union(torch.cat([bx,by,bw,bh], dim=-1),target[...,:4])\n",
    "\n",
    "        xy_loss = self.mse(torch.cat([bx,by], dim=-1), target[...,:2])\n",
    "        bwbh = torch.cat([bw,bh], dim=-1)\n",
    "        wh_loss = self.mse(torch.sqrt(torch.abs(bwbh)+1e-32),torch.sqrt(torch.abs(target[...,2:4])+1e-32))\n",
    "        obj_loss = self.mse(exist_mask,exist_mask*ious*torch.sigmoid(existing_boxes[...,4:5]))\n",
    "        #(ious.max(-1)[0]<0.6).int().unsqueeze(-1)\n",
    "        no_obj_loss =self.mse((1-exist_mask),\n",
    "                             (((1-exist_mask)*\n",
    "                                     (1-torch.sigmoid(predictions[...,4:5])))*\n",
    "                                     ((ious.max(-1)[0]<0.6).int().unsqueeze(-1)\n",
    "                             )))\n",
    "        class_loss = F.nll_loss((exist_mask*F.log_softmax(predictions[..., 5:],dim=-1)).flatten(end_dim=-2),target[..., 5:].flatten(end_dim=-2).argmax(-1))\n",
    "        return 5*xy_loss + 5*wh_loss + obj_loss + no_obj_loss + class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to Zhenliang He for the code\n",
    "#https://discuss.pytorch.org/t/is-there-any-layer-like-tensorflows-space-to-depth-function/3487/15\n",
    "class SpaceToDepth(nn.Module):\n",
    "    def __init__(self, block_size):\n",
    "        super().__init__()\n",
    "        self.bs = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, H // self.bs, self.bs, W // self.bs, self.bs)  # (N, C, H//bs, bs, W//bs, bs)\n",
    "        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n",
    "        x = x.view(N, C * (self.bs ** 2), H // self.bs, W // self.bs)  # (N, C*bs^2, H//bs, W//bs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters, kernel_size, stride, pad):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels = in_channels,\n",
    "                              out_channels = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              stride = stride,\n",
    "                              padding = pad,\n",
    "                              bias = False)\n",
    "        self.batchnorm = nn.BatchNorm2d(filters)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "    def forward(self, x):\n",
    "        x = self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloV2Model(pl.LightningModule):\n",
    "    def __init__(self, architechture=None, split_size=13, num_boxes=5, num_classes=20):\n",
    "        super(YoloV2Model, self).__init__()\n",
    "        self.S=split_size\n",
    "        self.B=num_boxes\n",
    "        self.C=num_classes\n",
    "        self.darknet_before_skip = self._create_conv(architechture[0],in_channels=3)\n",
    "        self.middle =  CNNBlock(in_channels = 512,\n",
    "                                filters = 64,\n",
    "                                kernel_size = 1,\n",
    "                                stride = 1,\n",
    "                                pad = 0)\n",
    "        self.space_to_depth = SpaceToDepth(block_size=2)\n",
    "        self.darknet_after_skip = self._create_conv(architechture[1],in_channels=512)\n",
    "        self.conv_end1 =  CNNBlock(in_channels = 1280,\n",
    "                                filters = 1024,\n",
    "                                kernel_size = 3,\n",
    "                                stride = 1,\n",
    "                                pad = 1)\n",
    "        self.final_conv =  nn.Conv2d(in_channels = 1024,\n",
    "                              out_channels = num_boxes*(4+1+num_classes),\n",
    "                              kernel_size = 1,\n",
    "                              stride = 1,\n",
    "                              padding = 0,\n",
    "                              bias = True)\n",
    "        self.loss = YoloV2Loss()\n",
    "        self.anchor_boxes = torch.tensor([[0,0,1.3221, 1.73145],[0,0,3.19275, 4.00944],[0,0,5.05587, 8.09892],[0,0,9.47112, 4.84053],[0,0,11.2364, 10.0071]],device=self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet_before_skip(x)\n",
    "        middle = self.middle(x)\n",
    "        middle = self.space_to_depth(middle)\n",
    "        x = self.darknet_after_skip(x)\n",
    "        x = torch.cat([middle,x],dim=1)\n",
    "        x = self.conv_end1(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        x = x.view(-1,self.S, self.S, self.B,4+1+self.C)\n",
    "        return x\n",
    "        \n",
    "    def _create_conv(self, architecture,in_channels):\n",
    "        layers = []\n",
    "\n",
    "        for x in architecture:\n",
    "            if \"Conv\" in str(type(x)):\n",
    "                layer = CNNBlock(in_channels = in_channels,\n",
    "                                    filters = x.filters,\n",
    "                                    kernel_size = x.kernel_size,\n",
    "                                    stride = x.stride,\n",
    "                                    pad = x.pad)\n",
    "                layers += [layer]\n",
    "                in_channels = x.filters\n",
    "\n",
    "            elif \"MaxPool\" in str(type(x)):\n",
    "                layers += [nn.MaxPool2d(kernel_size=(x.kernel_size, x.kernel_size), stride=(x.stride, x.stride))]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-5, weight_decay=configs.WEIGHT_DECAY)\n",
    "        return {'optimizer': optimizer}\n",
    "\n",
    "    def _calc_map(self, x, y, pred):\n",
    "        self.anchor_boxes = self.anchor_boxes.to(self.device)\n",
    "        exist_mask = torch.round(torch.sigmoid(pred[...,4:5]))\n",
    "        cell_idx = torch.arange(13,device=self.device)\n",
    "        bx = exist_mask*torch.sigmoid(pred[...,0:1]) + exist_mask*cell_idx.view([1,1,-1,1,1])\n",
    "        by = exist_mask*torch.sigmoid(pred[...,1:2]) + exist_mask*cell_idx.view([1,-1,1,1,1])\n",
    "        bw = exist_mask*self.anchor_boxes[:,2].view([1,1,1,-1,1]) * exist_mask*torch.exp(pred[...,2:3])\n",
    "        bh = exist_mask*self.anchor_boxes[:,3].view([1,1,1,-1,1]) * exist_mask*torch.exp(pred[...,3:4])\n",
    "        pred[...,:4]=torch.cat([bx,by,bw,bh],dim=-1)\n",
    "        pred_boxes, target_boxes = get_bboxesmine(x=x,y=y,predictions=pred,iou_threshold=0.45, threshold=0.005, S=self.S,B=self.B, device=self.device)\n",
    "        mean_avg_prec = mAP(pred_boxes,target_boxes,iou_threshold=0.5)\n",
    "        return mean_avg_prec\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred_y = self(x)\n",
    "        loss = YoloV2Loss()(pred_y, y, device=self.device,epoch=self.current_epoch)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        with torch.no_grad():\n",
    "            mAP = self._calc_map(x, y, pred_y.clone())\n",
    "            self.log('train_mAP', mAP, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred_y = self(x)\n",
    "        loss = YoloV2Loss()(pred_y, y, device=self.device,epoch=self.current_epoch)\n",
    "        self.log('valid_loss', loss, prog_bar=True)\n",
    "        mAP = self._calc_map(x.detach(), y.detach(), pred_y.detach())\n",
    "        self.log('valid_mAP', mAP,prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred_y = self(x)\n",
    "        loss = YoloV2Loss()(pred_y, y, device=self.device,epoch=self.current_epoch)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        mAP= self._calc_map(x.detach(), y.detach(), pred_y.detach())\n",
    "        self.log('test_mAP', mAP,prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    model = YoloV2Model(architechture = [architechture_config1,architechture_config2], split_size=13, num_boxes=5, num_classes=20)\n",
    "    data = YoloV2DataModule()\n",
    "    for p in model.darknet_before_skip.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.darknet_after_skip.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.middle.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.conv_end1.parameters():\n",
    "        p.requires_grad = False\n",
    "    trainer = pl.Trainer(gpus=1,max_epochs=10)\n",
    "    trainer.fit(model,datamodule=data)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
